<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Yichong Lu


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link
  defer
  rel="stylesheet"
  type="text/css"
  href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&display=swap"
>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="/assets/css/jekyll-pygments-themes/github.css">

<!-- Styles -->

<!--<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>/assets/img/xd.png</text></svg>">-->
<link rel="icon" href="/assets/img/xd.png">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">




  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
          <a href="mailto:%79%69%79%69.%6C%69%61%6F@%7A%6A%75.%65%64%75.%63%6E"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.co.kr/citations?user=BUrQL24AAAAJ&hl=ja" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/yichonglu" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>


        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
                    
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

  <div class="container mt-5">
    <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     Yichong Lu
    </h1>
     <p class="desc">Zhejiang University</p>
  </header>

  <article>
    
    <div class="profile float-right">
      <img class="img-fluid z-depth-1 rounded" src="" srcset="/assets/img/prof_pic.jpg 384w">
    </div>
    

    <div class="clearfix">
      <p>I am a second-year Master's student in <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a> supervised by <a href="https://yiyiliao.github.io/" target="_blank" rel="noopener noreferrer">Yiyi Liao</a> and Huimin Yu. I was lucky to have research collaboration with <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Prof. Andreas Geiger</a>. Before that, I obtained my Bachelor degree in <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a> at 2023.</p>

      <p>My research interest lies in 3D computer vision, including scene understanding, 3D reconstruction and 3D generative models.</p>

      <p>Feel free to contact me via <a href="mailto:yc.lu@zju.edu.cn">email</a> for cooperation!</p>
      
     </div>
    
    <div class="news">
      <h2>news</h2>
      
        <div class="table-responsive" style="height: 250px; overflow-y:scroll">
          <table class="table table-sm table-borderless" style="width: 100%">
            <colgroup>
               <col span="1" style="width: 15%;">
               <col span="1" style="width: 85%;">
            </colgroup>
          <!-- Put <thead>, <tbody>, and <tr>'s here! -->
          <tbody>
            
              <tr>
                <th scope="row">Feb 27, 2025</th>
                <td>
                  
                    <a href="https://xdimlab.github.io/UrbanCAD/" target="_blank" rel="noopener noreferrer">UrbanCAD</a> is accepted to CVPR 2025.
                  
                </td>
              </tr>        
          </tbody>
          </table>
        </div>
    </div>
    

    <div class="publications">
      <h2>selected publications</h2>
      <p> Full publication list can be found on <a href="https://scholar.google.co.kr/citations?user=BUrQL24AAAAJ&hl=ja" target="_blank" rel="noopener noreferrer">Google Scholar</a>. <br>
      <sup>*</sup>equal contribution; <sup>♯</sup>corresponding author. </p>

      <ol class="bibliography">
        <li>
        <div class="row">
          <div class="col-md-3">
            
            <div class="img-fluid rounded">
              <img src="/assets/teaser/UrbanCAD_demo.gif" alt="Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation" style="width: 100%;">
            </div>
            
          </div>
        
          <div id="Lu2025CVPR" class="col-md-9">
                <div class="title">UrbanCAD: Towards Highly Controllable and Photorealistic 3D Vehicles for Urban Scene Simulation</div>
                      <div class="author">
        
                          <em>Lu, Yichong</em>,

                          <a target="_blank" rel="noopener noreferrer">Cai, Yichi</a>,
                        
                          <a href="https://zhanghe3z.github.io/" target="_blank" rel="noopener noreferrer">Zhang, Shangzhan<sup>*</sup></a>,
                        
                          <a target="_blank" rel="noopener noreferrer">Zhou, Hongyu</a>,
        
                          <a target="_blank" rel="noopener noreferrer">Hu, Haoji</a>,

                          <a target="_blank" rel="noopener noreferrer">Yu, Huimin</a>,
        
                          <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
        
                        and  <a href="https://yiyiliao.github.io/" target="_blank" rel="noopener noreferrer">Liao, Yiyi<sup>♯</sup></a>
        
                      </div>
        
                      <div class="periodical">
                      
                        CVPR 2025

                      
                      </div>
            
        
                      <div class="links">
                      
                        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>

                        <a href="https://arxiv.org/pdf/2411.19292" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                  
                        <a href="https://xdimlab.github.io/UrbanCAD/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
                      
                      </div>
        
                      <!-- Hidden abstract block -->
                      
                      <div class="abstract hidden">
                        <p>Photorealistic 3D vehicle models with high controllability are essential for autonomous driving simulation and data augmentation. While handcrafted CAD models provide flexible controllability, free CAD libraries often lack the high-quality materials necessary for photorealistic rendering. Conversely, reconstructed 3D models offer high-fidelity rendering but lack controllability. In this work, we introduce UrbanCAD, a framework that pushes the frontier of the photorealism-controllability trade-off by generating highly controllable and photorealistic 3D vehicle digital twins from a single urban image and a large collection of free 3D CAD models and handcrafted materials. These digital twins enable realistic 360 degrees rendering, vehicle insertion, material transfer, relighting, and component manipulation such as opening doors and rolling down windows, supporting the construction of long-tail scenarios. To achieve this, we propose a novel pipeline that operates in a retrieval-optimization manner, adapting to observational data while preserving fine-grained handcrafted properties including component-level controllability, complex material physical properties, interior geometry, and detailed material assignment. Furthermore, given multi-view background perspective and fisheye images, we approximate environment lighting using fisheye images and reconstruct the background with 3DGS, enabling the photorealistic insertion of optimized CAD models into rendered novel view backgrounds. Experimental results demonstrate that UrbanCAD outperforms baselines based on reconstruction and retrieval in terms of photorealism. Additionally, we show that various perception models maintain their accuracy when evaluated on UrbanCAD with in-distribution configurations but degrade when applied to realistic out-of-distribution data generated by our method. This suggests that UrbanCAD is a significant advancement in creating photorealistic, safety-critical driving scenarios for downstream applications.</p>
                      </div>
            
        
                      <!-- Hidden bibtex block -->
            
          </div>
        </div>
        </li>

          <li>
        <div class="row">
          <div class="col-md-3">

            <div class="img-fluid rounded">
              <img src="/assets/teaser/panopticnerf360.gif" alt="PanopticNeRF-360: Panoramic 3D-to-2D Label Transfer in Urban Scenes" style="width: 100%;">
            </div>

          </div>

          <div id="Fu2025TPAMI" class="col-md-9">
                <div class="title">PanopticNeRF-360: Panoramic 3D-to-2D Label Transfer in Urban Scenes</div>
                      <div class="author">

                          <a href="https://fuxiao0719.github.io/" target="_blank" rel="noopener noreferrer">Fu, Xiao<sup>*</sup></a>,

                          <a href="https://zhanghe3z.github.io/" target="_blank" rel="noopener noreferrer">Fu, Zhang, Shangzhan<sup>*</sup></a>,

                          <a href="https://tianrun-chen.github.io/" target="_blank" rel="noopener noreferrer">Chen, Tianrun</a>,

                          <em>Lu, Yichong</em>,

                          <a href="https://xzhou.me/" target="_blank" rel="noopener noreferrer">Zhou, Xiaowei</a>,

                          <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,

                        and  <a href="https://yiyiliao.github.io/" target="_blank" rel="noopener noreferrer">Liao, Yiyi<sup>♯</sup></a>

                      </div>

                      <div class="periodical">

                        TPAMI 2025



                      </div>


                      <div class="links">

                        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>

                        <a href="https://arxiv.org/pdf/2309.10815" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>

                        <a href="https://github.com/fuxiao0719/PanopticNeRF/tree/panopticnerf360" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>

                        <a href="https://fuxiao0719.github.io/projects/panopticnerf360/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>

                      </div>

                      <!-- Hidden abstract block -->

                      <div class="abstract hidden">
                        <p>Training perception systems for self-driving cars requires substantial 2D annotations that are labor-intensive to manual label. While existing datasets provide rich annotations on pre-recorded sequences, they fall short in labeling rarely encountered viewpoints, potentially hampering the generalization ability for perception models. In this paper, we present PanopticNeRF-360, a novel approach that combines coarse 3D annotations with noisy 2D semantic cues to generate high-quality panoptic labels and images from any viewpoint. Our key insight lies in exploiting the complementarity of 3D and 2D priors to mutually enhance geometry and semantics. Specifically, we propose to leverage coarse 3D bounding primitives and noisy 2D semantic and instance predictions to guide geometry optimization, by encouraging predicted labels to match panoptic pseudo ground truth. Simultaneously, the improved geometry assists in filtering 3D&amp;2D annotation noise by fusing semantics in 3D space via a learned semantic field. To further enhance appearance, we combine MLP and hash grids to yield hybrid scene features, striking a balance between high-frequency appearance and contiguous semantics. Our experiments demonstrate PanopticNeRF-360’s state-of-the-art performance over label transfer methods on the challenging urban scenes of the KITTI-360 dataset. Moreover, PanopticNeRF-360 enables omnidirectional rendering of high-fidelity, multi-view and spatiotemporally consistent appearance, semantic and instance labels.</p>
                      </div>


                      <!-- Hidden bibtex block -->

          </div>
        </div>

      </li>

        <li>
        <div class="row">
          <div class="col-md-3">
            
            <div class="img-fluid rounded">
              <img src="/assets/teaser/3dv2022.gif" alt="Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation" style="width: 100%;">
            </div>
            
          </div>
        
          <div id="Fu2022THREEDV" class="col-md-9">
                <div class="title">Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation</div>
                      <div class="author">
        
                          <a href="https://fuxiao0719.github.io/" target="_blank" rel="noopener noreferrer">Fu, Xiao<sup>*</sup></a>,
                        
                          <a href="https://zhanghe3z.github.io/" target="_blank" rel="noopener noreferrer">Fu, Zhang, Shangzhan<sup>*</sup></a>,
                        
                          <a href="https://tianrun-chen.github.io/" target="_blank" rel="noopener noreferrer">Chen, Tianrun</a>,
        
                          <em>Lu, Yichong</em>,
                        
                          <a href="https://lanyunzhu.site/" target="_blank" rel="noopener noreferrer">Zhu, Lanyun</a>,
        
                          <a href="https://xzhou.me/" target="_blank" rel="noopener noreferrer">Zhou, Xiaowei</a>,
        
                          <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
        
                        and  <a href="https://yiyiliao.github.io/" target="_blank" rel="noopener noreferrer">Liao, Yiyi<sup>♯</sup></a>
        
                      </div>
        
                      <div class="periodical">
                      
                        3DV 2022
                      

                      
                      </div>
            
        
                      <div class="links">
                      
                        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>

                        <a href="https://www.cvlibs.net/publications/Fu2022THREEDV.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                  
                        <a href="https://github.com/fuxiao0719/PanopticNeRF" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
                    
                        <a href="https://www.cvlibs.net/publications/Fu2022THREEDV_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
                  
                        <a href="https://fuxiao0719.github.io/projects/panopticnerf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
                      
                      <a href="https://www.youtube.com/watch?v=5QKTeFLciWo" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
                      
                      </div>
        
                      <!-- Hidden abstract block -->
                      
                      <div class="abstract hidden">
                        <p>Large-scale training data with high-quality annotations is critical for training semantic and instance segmentation models. Unfortunately, pixel-wise annotation is labor-intensive and costly, raising the demand for more efficient labeling strategies. In this work, we present a novel 3D-to-2D label transfer method, Panoptic NeRF, which aims for obtaining per-pixel 2D semantic and instance labels from easy-to-obtain coarse 3D bounding primitives. Our method utilizes NeRF as a differentiable tool to unify coarse 3D annotations and 2D semantic cues transferred from existing datasets. We demonstrate that this combination allows for improved geometry guided by semantic information, enabling rendering of accurate semantic maps across multiple views. Furthermore, this fusion process resolves label ambiguity of the coarse 3D annotations and filters noise in the 2D predictions. By inferring in 3D space and rendering to 2D labels, our 2D semantic and instance labels are multi-view consistent by design. Experimental results show that Panoptic NeRF outperforms existing label transfer methods in terms of accuracy and multi-view consistency on challenging urban scenes of the KITTI-360 dataset.</p>
                      </div>
            
        
                      <!-- Hidden bibtex block -->
            
          </div>
        </div>
        
      </li>

    </div>

  </article>

</div>
  </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2024 Yichong  Lu.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.
    
    Last updated: November 15, 2024.
    
  </div>
</footer>
  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>

<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
